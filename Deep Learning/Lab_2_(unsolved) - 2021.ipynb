{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VT-N4Kx-ZbQu"
   },
   "source": [
    "<font size=25>Laboratory 2 summary</font>\n",
    "\n",
    "In this lab you will:\n",
    "* Learn how the `torch.autograd` package tracks the computation graph and performs backpropagation\n",
    "* Train a Linear Regressor using Gradient Descent\n",
    "* Define a Neural Network by hand and train it on a dataset\n",
    "* Learn how to use `torch.nn.Module` and all its adjacent helpers to define a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VFxGzPYv4xl"
   },
   "source": [
    "# **Part I: Autograd**\n",
    "\n",
    "|![Computational Graph](https://i.ibb.co/zr7TV4H/computational-graph.png)|\n",
    "|:--:|\n",
    "| Example of a simple computational graph. Autograd computes df/dx, df/dy, df/dz. [Source: CS231, lecture 4](http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture04.pdf) |\n",
    "\n",
    "The **torch.autograd** package provides automatic differentiation for all operations on Tensors. It is a *define-by-run* framework (the computational graph is *dynamic*), which means that your backprop is defined by how your code is run, and that every single iteration can be different (as opposed to TensorFlow, where you first define the graph and then run it - a *static graph*).\n",
    "\n",
    "If you set the Tensor attribute `.requires_grad` to `True`, it starts to track all operations on it. When you finish your computation you can call `.backward()` and have all the gradients computed automatically. The gradient for this tensor will be accumulated into `.grad` attribute.\n",
    "\n",
    "There’s one more class which is very important for autograd implementation - a `Function`.\n",
    "\n",
    "`Tensor` and `Function` are interconnected and build up an acyclic graph, that encodes a complete history of computation. Each tensor has a `.grad_fn` attribute that references a Function that has created the Tensor (except for Tensors created by the user - their `grad_fn` is `None`).\n",
    "\n",
    "If you want to compute the derivatives, you can call `.backward()` on a Tensor. If Tensor is a scalar (i.e. it holds a one element data), you don’t need to specify any arguments to `backward`, however if it has more elements, you need to specify a gradient argument that is a tensor of matching shape.\n",
    "\n",
    "*Ref: [autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJmk5NuSeiTj"
   },
   "source": [
    "We can use `torchviz` to visualise the computation history. It displays the computation graph, coloring the leaf nodes in blue, the root (the element whose graph we are plotting in green), and all other intermediate operations in grey.\n",
    "\n",
    "First we would need to install and import it in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MiCa79zYdZXY"
   },
   "outputs": [],
   "source": [
    "!pip install torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6JPnw8SYxW8"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from IPython.display import HTML\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation, rc\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchviz\n",
    "from typing import Iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eS8uYE5NfElK"
   },
   "source": [
    "## Viewing the computational graph\n",
    "\n",
    "Let us define a simple Tensor, specifying the `requires_grad` parameter, and then draw the computation graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGXw1Z_5yKp2"
   },
   "outputs": [],
   "source": [
    "a = torch.ones(2, 2, requires_grad=True)\n",
    "print(\"a = \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v6cA_lzHyvpX"
   },
   "outputs": [],
   "source": [
    "b = torch.zeros(2, 2, requires_grad=True)\n",
    "c = a + b\n",
    "print(\"b = \", b)\n",
    "print(\"c = \", c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivSzB2Hyn447"
   },
   "source": [
    "Notice how the `grad_fn` attribute is now populated with `AddBackward0`. This means that `autograd` knows that the tensor `c` results from the addition operation between tensor `a` and `b`. To compute the gradients, autograd will traverse the computational graph from the root node (in this case, `c`) to its leaves (`a` and `b`).\n",
    "\n",
    "Let's visualize the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ASfvD_nn2pS"
   },
   "outputs": [],
   "source": [
    "torchviz.make_dot(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bA-Nsl_kfXk4"
   },
   "source": [
    "Note that, had we not specified `requires_grad`, the computation history would not have been tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SLf4czA7gTvQ"
   },
   "outputs": [],
   "source": [
    "a = torch.ones(2, 2)\n",
    "b = torch.zeros(2, 2)\n",
    "c = a + b\n",
    "print(\"c = \", c)\n",
    "torchviz.make_dot(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCi8pdbrgUOa"
   },
   "source": [
    "Let's look at another example, involving more tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xn4tcEFCfne3"
   },
   "outputs": [],
   "source": [
    "a = torch.randn(5, 2, requires_grad=True)\n",
    "b = torch.randn(2, 7, requires_grad=True)\n",
    "c = torch.randn(5, 7, requires_grad=True)\n",
    "\n",
    "x = torch.tanh(a @ b + c).sum()\n",
    "\n",
    "print(x)\n",
    "torchviz.make_dot(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BTyxX2dohsIq"
   },
   "source": [
    "## Computing the gradient of the result with respect to all tensors in the graph\n",
    "Let's consider a simple dot product multiplication\n",
    " $y = \\textbf{w}^T \\textbf{x}$ and plot it's computational graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DxGtfXSqywyL"
   },
   "outputs": [],
   "source": [
    "w = torch.rand([3,1], requires_grad=True)\n",
    "x = torch.Tensor([[10, 11, 12]])\n",
    "y = x @ w # equivalent to y = torch.matmul(w, x)\n",
    "print(f'w = {w}')\n",
    "print(f'x = {x}')\n",
    "print(f'y = {y}')\n",
    "torchviz.make_dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p37w6dzZtIRJ"
   },
   "source": [
    "We compute the gradient of y with respect to **w**: $\\nabla_\n",
    "\\textbf{w} y = [\\frac{\\partial y}{\\partial w_1}, \\frac{\\partial y}{\\partial w_2}, \\frac{\\partial y}{\\partial w_3}] = [x_1, x_2, x_3] = \\textbf{x}$\n",
    "\n",
    "To do this in PyTorch, we simply call `backward()` on Tensor `y`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rw6jmkw9zNFe"
   },
   "outputs": [],
   "source": [
    "y.backward()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wE9RoRZJ6bwu"
   },
   "source": [
    "The gradient of y with respect to **w** will be stored in `w.grad`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LEwYN7Mk7J6x"
   },
   "outputs": [],
   "source": [
    "print(\"Gradient of y wrt w = \", w.grad)\n",
    "print(\"Gradient of y wrt x = \", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QiX5c_eO8GB3"
   },
   "source": [
    "What is the gradient of y with respect to **x** and why was it not stored in `x.grad`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRQXufAFiFZS"
   },
   "source": [
    "**Important:** Calling `.backward()` erases the forward graph by default. Calling `.backward()` again will raise an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EgbCrW48zvI6"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    y.backward()\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cOYaESEMIpeS"
   },
   "source": [
    "There are times when this is undesirable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_pfVeVd10Jnv"
   },
   "outputs": [],
   "source": [
    "a = torch.rand([1,3], requires_grad=True)\n",
    "b = torch.rand([3,2])\n",
    "c = torch.rand([1])\n",
    "\n",
    "node1 = (a @ b).sum()\n",
    "node2 = node1 * c\n",
    "\n",
    "print(f'node1.shape = {node1.shape}')\n",
    "\n",
    "print(\"node1 graph:\")\n",
    "torchviz.make_dot(node1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ysL1sBqexZ2S"
   },
   "outputs": [],
   "source": [
    "print(f'node2.shape = {node2.shape}')\n",
    "print(\"node2 graph:\")\n",
    "torchviz.make_dot(node2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWhCwKKwxfZV"
   },
   "source": [
    "Notice how the first graph is a subgraph of the second one. When calling the first `.backward()`, the computational graph will get reset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rpgoPttnxHTc"
   },
   "outputs": [],
   "source": [
    "node1.backward() # now the computation graph gets reset\n",
    "node2.backward() # this will throw the error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwT8hEgDJAbD"
   },
   "source": [
    "To override this behavior, you can call `.backward(retain_graph=True)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AVTfw39nJ1B2"
   },
   "outputs": [],
   "source": [
    "a = torch.rand([1,3], requires_grad=True)\n",
    "b = torch.rand([3,2])\n",
    "c = torch.rand([1])\n",
    "\n",
    "node1 = (a @ b).sum()\n",
    "node2 = node1 * c\n",
    "\n",
    "print(f'node1.shape = {node1.shape}')\n",
    "print(f'node2.shape = {node2.shape}')\n",
    "\n",
    "node1.backward(retain_graph=True) # Prevent the erasure of the computation graph\n",
    "print(a.grad)\n",
    "node2.backward() # Now this will work\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4GEzMJ18zDAr"
   },
   "source": [
    "We might want to perform operations to our Tensors that do not contribute to the training of the model (the most common example is calculating the loss functions for test data while training). We can make sure that the computational graph is not stored by wrapping our code in `torch.no_grad()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Q7PThpIybba"
   },
   "outputs": [],
   "source": [
    "a = torch.randn(3, requires_grad=True)\n",
    "with torch.no_grad():\n",
    "  b = a + 1\n",
    "\n",
    "torchviz.make_dot(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ZtfWU440UTr"
   },
   "source": [
    "## Exercise 1: Linear regression with autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YwAR6tVpz9xK"
   },
   "source": [
    "Recall the linear regression exercise - in our previous lab we've trained our model by computing the closed-form solution. Let's do a linear regression using Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MA5Jy4w6eWW"
   },
   "source": [
    "We'd like to find the global minimum of the error function $E(w)$ - i.e. we want to find a *weight vector $w$* which minimizes $E(w)$ (\\*).\n",
    "\n",
    "Let $\\tau$ be our iteration step. We want to perform *Gradient Descent* on the loss surface - the simplest approach to using gradient information is to update the weights by making small steps in the direction of the negative gradient (see Chapter 5.2 - Bishop) [[1]]: \n",
    "\n",
    "$$w^{\\tau+1}=w^\\tau - \\eta\\nabla E(w^\\tau)$$\n",
    "\n",
    "The $\\eta$ parameter is known as the *learning rate* and it represents the magnitude of the gradient step.\n",
    "\n",
    "We're going to use the *Mean Squared Error (MSE)* loss for the Linear Regression. Recall that the MSE Loss is defined as follows:\n",
    "\n",
    "$$MSE(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=0}^{n} (y_i - \\hat{y_i})^2 $$\n",
    "\n",
    "Where $n$ is the dataset length, $y$ is the true label vector and $\\hat{y}$ is the prediction vector.\n",
    "\n",
    "(\\*): *In the context of Neural Networks, this won't be always possible (or desirable - recall the polynomial that was overfitting our data in the previous lab). The loss surface of a Neural Network is a lot more complex - there could be saddle points, parts of the surface with almost no gradients or highly irregular neighbours. We'll (usually) find a \"good enough\" local minima, improving our chances of finding it by using various optimization tehniques.*\n",
    "\n",
    "[1]: http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAICYqjNsFf0"
   },
   "source": [
    "**TODO:**\n",
    "\n",
    "1) Implement the `__call__()` method of the `MSE` class. This method will compute the MSE Loss between predictions in `y` and ground-truth values in `target`\n",
    "\n",
    "2) Implement the `forward()` method for the `GDLinearRegression` class. This method will predict our points w.r.t. the weights and biases.\n",
    "\n",
    "3) Impement the `step()` method for the `GD` class. This method will update perform a gradient descend update to the parameters.\n",
    "\n",
    "4) Implement the `train()` routine:\n",
    "\n",
    "- Make a prediction with your `GDLinearRegression` model.\n",
    "- Calculate the MSE Loss using the `MSE` object.\n",
    "- Calculate the gradients by using `.backward()` on the loss.\n",
    "- Do an optimizer `.step()` and reset the gradients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LgLn541Nw8m1"
   },
   "outputs": [],
   "source": [
    "class MSE():\n",
    "  \"\"\"The Mean Squared Error loss\"\"\"\n",
    "  \n",
    "  def __call__(self, y: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    TODO: compute the MSE between predictions in tensor y and ground truth \n",
    "    values in tensor target. Both tensors are unidimensional.\n",
    "    y: tensor of size N containing the predictions \n",
    "    target: tensor of size N containing the ground-truth values\n",
    "    \"\"\"\n",
    "    mse = ... # Todo\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dTU7B508cns"
   },
   "source": [
    "Note that we're inheriting `nn.Module`. In this specific case, the only reason for doing this is that the `nn.Module` class provides us with the `.parameters()` class method (used when instantiating the optimizer). \n",
    "\n",
    "Note: In the last exercise you will also inherit `nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hOd2nj0_xqsR"
   },
   "outputs": [],
   "source": [
    "class GDLinearRegression(nn.Module):\n",
    "  \"\"\"A simple Linear Regression model\"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    # We're initializing our model with random weights\n",
    "    self.w = nn.Parameter(torch.randn(1, requires_grad = True))\n",
    "    self.b = nn.Parameter(torch.randn(1, requires_grad = True))\n",
    "\n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    TODO: implement feedforwad call for a simple linear regression:\n",
    "      y = wx + b\n",
    "    Arguments: x is tensor of size (num_examples x 1)\n",
    "    \"\"\"\n",
    "    y = ... # Todo\n",
    "    return y\n",
    "\n",
    "  # PyTorch is accumulating gradients\n",
    "  # After each Gradient Descent step we should reset the gradients\n",
    "  def zero_grad(self):\n",
    "    self.w.grad.zero_()\n",
    "    self.b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UEh2X8-oESU7"
   },
   "outputs": [],
   "source": [
    "class GD:\n",
    "  \"\"\"\n",
    "  Gradient Descent optimizer\n",
    "  We will write our own class for now, but in the future we will use\n",
    "  the Optimizers implemented in Pytorch:\n",
    "  https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer \n",
    "  \"\"\"\n",
    "  def __init__(self, params: Iterator[nn.Parameter], lr: float):\n",
    "    self.w, self.b = list(params)\n",
    "    self.lr = lr\n",
    "\n",
    "    # We'll use these two for a plot :)\n",
    "    if len(self.w.shape) == 1:\n",
    "      self.w_hist = [self.w.item()]\n",
    "      self.b_hist = [self.b.item()]\n",
    "\n",
    "  def step(self):\n",
    "    \"\"\"\n",
    "    Perform a gradient decent step. Update the parameters w and b by using:\n",
    "     - the gradient of the loss with respect to the parameters\n",
    "     - the learning rate\n",
    "    This method is called after backward(), so the gradient of the loss wrt \n",
    "    the parameters is already computed (but where is it stored?)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "      self.w -= ... # Todo\n",
    "      self.b -= ... # Todo\n",
    "\n",
    "    # We'll use these two for a plot :)\n",
    "    if len(self.w.shape) == 1:\n",
    "      self.w_hist.append(self.w.item())\n",
    "      self.b_hist.append(self.b.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6iDjZ_O0kD3"
   },
   "outputs": [],
   "source": [
    "def train(model: GDLinearRegression, data: torch.Tensor,\n",
    "          labels: torch.Tensor, optim: GD, criterion: MSE):\n",
    "  \"\"\"Linear Regression train routine\"\"\"\n",
    "  # TODO forward pass: compute predictions (hint: use model.forward)\n",
    "  predictions = ... \n",
    "  \n",
    "  # TODO forward pass: compute loss (hint: use criterion)\n",
    "  loss = ... \n",
    "\n",
    "  # TODO backpropagation: compute gradients of loss wrt weights\n",
    "  # ...\n",
    "  \n",
    "  # TODO GD step: update weights using the gradients (hint: use optim)\n",
    "  # ...\n",
    "\n",
    "  # TODO reset the gradients (hint: use model)\n",
    "  # ...\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8CX9WsgkyfwN"
   },
   "source": [
    "Let's get some data and plot the *closed-form Linear Regressor*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u-ECH8gZye_M"
   },
   "outputs": [],
   "source": [
    "# number of dataset points\n",
    "N = 73 #@param {type:\"slider\", min:10, max:100, step:1}\n",
    "\n",
    "X, y, coef = make_regression(n_samples = N, n_features=1, noise=20, coef=True)\n",
    "closed_form = LinearRegression().fit(X, y)\n",
    "cf_prediction = closed_form.predict(X)\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X, cf_prediction, color='red', label='Closed form')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "X = torch.Tensor(X)\n",
    "y = torch.Tensor(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxdRabGuwmg5"
   },
   "source": [
    "Now let's plot the Linear Regressor obtained by Gradient Descent, at each optimization step.\n",
    "\n",
    "Feel free to play with the learning rate and total optimization steps and see how the results compare.\n",
    "\n",
    "What happens if you set a (very) big learning rate? (>1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0HZIFkFwTUS"
   },
   "outputs": [],
   "source": [
    "lr = 0.781 #@param {type: \"slider\", min: 0.001, max: 2, step: 0.005}\n",
    "total_steps = 100 #@param {type:\"slider\", min: 0, max: 100, step: 1}\n",
    "\n",
    "model = GDLinearRegression()\n",
    "optimizer = GD(model.parameters(), lr=lr)\n",
    "criterion = MSE()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.close()\n",
    "\n",
    "gd_axis, = ax.plot([], [], label='Gradient Descent')\n",
    "cf_axis, = ax.plot([], [], label='Closed Form')\n",
    "legend = ax.legend(loc=2, prop={'size': 10})\n",
    "\n",
    "def init():\n",
    "  scatter = ax.scatter(X, y)\n",
    "  cf_axis.set_data(X, cf_prediction)\n",
    "  return (scatter, cf_axis,)\n",
    "\n",
    "def train_and_plot(step: int):\n",
    "  with torch.no_grad():\n",
    "    y_pred = model(X)\n",
    "  gd_axis.set_data(X, y_pred)\n",
    "  \n",
    "  train(model, X, y, optimizer, criterion)\n",
    "  \n",
    "  legend.texts[0].set_text(f\"Gradient Descent (step={step})\")\n",
    "  legend.texts[1].set_text(\"Closed Form\")\n",
    "\n",
    "  return (gd_axis, legend, )\n",
    "\n",
    "anim = animation.FuncAnimation(\n",
    "    fig, train_and_plot, init_func=init, \n",
    "    frames=total_steps, interval=total_steps*2, blit=True\n",
    "    )\n",
    "\n",
    "rc('animation', html='jshtml')\n",
    "anim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8tm_zDdDkW1"
   },
   "source": [
    "Let's see how the weight `w` and bias `b` changed after every Gradient Descent step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2_YRRflDCVPG"
   },
   "outputs": [],
   "source": [
    "plt.plot(optimizer.w_hist, label='Weight', color='fuchsia')\n",
    "plt.plot(optimizer.b_hist, label='Bias', color='green')\n",
    "plt.xlabel('Gradient Descent steps')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xp47fj4W6GkX"
   },
   "source": [
    "# Part II: Neural Networks, finally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1v0N8Gc2kuF7"
   },
   "source": [
    "## Defining some clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eexHCBoTuYC2"
   },
   "source": [
    "We want to train a model that can learn to classify 2D points sampled from 2 clusters in a plane, for example:\n",
    "\n",
    "![Plot showing the distribution](https://i.imgur.com/XwavfqU.png)\n",
    "\n",
    "These clusters are 2D Normal Distributions, $\\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$, where $\\boldsymbol{\\mu} \\in \\mathbb{R}^2$ defines the center of the distribution, and $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{2 \\times 2}$ is a diagonal matrix containing the standard deviations over each direction. In other words, $\\boldsymbol{\\Sigma} = \\text{diag}(\\boldsymbol{\\sigma^2})$, where $\\sigma_i^2 \\in [0, +\\infty)$ for all $i = \\overline{1, 2}$ (See Chapter 2.3 - Bishop) [[2]](https://en.wikipedia.org/wiki/Multivariate_normal_distribution). \n",
    "\n",
    "Let's first define a helper function that will help us plot our 2D data, along with a class that implements a `sample` and a `plot` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_0PSlMVw6Gke"
   },
   "outputs": [],
   "source": [
    "def plot_set(data: np.array, labels: np.array, alpha=1):\n",
    "    \"\"\"Helper function that plots labeled data\"\"\"\n",
    "    plt.scatter(data[:,0], data[:,1], c=labels, cmap='Accent', alpha=alpha)\n",
    "\n",
    "class LabeledDistribution:\n",
    "    \"\"\"This class represents a distribution over (example, label) pairs\"\"\"\n",
    "\n",
    "    def sample(self, n: int) -> tuple:\n",
    "        \"\"\"Sample from the distribution\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def plot(self, n: int = 1000):\n",
    "        \"\"\"Plot `n` values sampled from the distribution\"\"\"\n",
    "        data, labels = self.sample(n)\n",
    "        plot_set(data, labels.type(torch.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1vaMdPxHzqpu"
   },
   "source": [
    "Now let's define a class that helps us define and sample from clusters in a plane. In its initialiser, `mu`, `sigma` and `labels` are lists of the same length as the number of clusters. `mu[i]` and `sigma[i]` define a cluster of label `labels[i]`. In other words, they define a distribution $\\mathcal{N}(\\boldsymbol{\\mu_i}, diag(\\boldsymbol{\\sigma_i}))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eB8v3mfvyf_a"
   },
   "outputs": [],
   "source": [
    "class Clusters(LabeledDistribution):\n",
    "    \"\"\"This class defines labeled normal-distributed clusters in a plane with diagonal covariance matrix\"\"\"\n",
    "    def __init__(self, mu: list = [[-2, -2], [2, 2]], \n",
    "                       sigma: list = [[1, 1], [1, 1]],\n",
    "                       labels: list = [0, 1]):\n",
    "        self._mu = torch.Tensor(mu)\n",
    "        self._sigma = torch.Tensor(sigma)\n",
    "        self._labels = torch.Tensor(labels).type(torch.long)\n",
    "\n",
    "        self.no_cls = len(set(labels)) # the number of classes in our distribution\n",
    "        self.no_clusters = self._mu.shape[0] # the number of clusters in our distribution\n",
    "        \n",
    "    def sample(self, n: int) -> tuple:\n",
    "        data = torch.normal(torch.zeros(n, 2), torch.ones(n, 2)) # take n samples from a standard normal distribution\n",
    "        cluster_idx = torch.randint(0, self.no_clusters, [n]) # randomly assign each observation to a cluster\n",
    "\n",
    "        shifted_data = data * self._sigma[cluster_idx] + self._mu[cluster_idx] # transform each observation such that they are \n",
    "                                                                               # sampled from the distribution of the cluster\n",
    "        labels = self._labels[cluster_idx] # get the label of each point by looking at the label of its cluster\n",
    "\n",
    "        return shifted_data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cV7jrPSyG6ke"
   },
   "source": [
    "Now we can sample points from our clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yh2EIObaHCml"
   },
   "outputs": [],
   "source": [
    "dist = Clusters()\n",
    "points, labels = dist.sample(10)\n",
    "for point, label in zip(points, labels):\n",
    "    x = round(point[0].item(), 2)\n",
    "    y = round(point[1].item(), 2)\n",
    "    print(f'({x}, {y}) belongs to class {label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bljhp1V7InRw"
   },
   "source": [
    "Let's now plot our clusters to make sure we achieved what we wanted in the first place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fGHRDyoO6Gk3"
   },
   "outputs": [],
   "source": [
    "# We now have a distribution to sample from :)\n",
    "dist = Clusters()\n",
    "dist.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-slwOdB5_A1"
   },
   "source": [
    "## Single Layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0inFYZWa5_Bc"
   },
   "source": [
    "### Model definition\n",
    "Let's build our first Neural Network in Pytorch, containing (for now) a single layer of perceptrons. Note that in the first example we manually construct our **Linear** layers (weights + biases) for didactic reasons, but you will use Pytorch's `torch.nn.Linear` module from now on.\n",
    "\n",
    "<!-- First, recall that you can define a **perceptron** as:\n",
    "$$y(x) = \\sigma(\\boldsymbol{w}^\\intercal \\boldsymbol{x} + b)$$ -->\n",
    "\n",
    "First, recall that **a layer of perceptrons** can be expressed as:\n",
    "$$\\boldsymbol{f}(x) = \\sigma(\\boldsymbol{W} \\boldsymbol{x} + \\boldsymbol{b})$$\n",
    "\n",
    "To build a Neural Network in Pytorch we subclass ``nn.Module`` and implement the following methods:\n",
    "1. `__init__` (the initialiser), where we initialise our weights and biases, as well as define our activation function.\n",
    "2. `forward`, which gets called under the hood when we run `model(...)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Crfn8GmP6vDa"
   },
   "source": [
    "### Using `nn.Module` to define Neural Networks\n",
    "In practice, defining our Neural Networks by hand would be obviously cumbersome, as well as a huge waste of time. Therefore, we use `torch.nn` [[3]](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) objects as helpers.\n",
    "\n",
    "We can therefore define our Neural Network by deriving the `nn.Module` class, which will give us access to a range of methods, like `.forward()`, `.train()`, `.zero_grad()`, `.eval()`, `.parameters()` etc. For the individual layers, we will simply instantiate a `nn.Linear` [[4]](https://pytorch.org/docs/stable/nn.html?highlight=nn%20linear#torch.nn.Linear) object.\n",
    "\n",
    "Even though we used our own class called `GD` before, in practice we use an optimiser instantiated from `torch.optim`, providing the model parameters, the learning rate and the regularization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wz3_4UPQ5_Bh"
   },
   "outputs": [],
   "source": [
    "class OneLayer(nn.Module):\n",
    "  \"\"\"\n",
    "  Single-Layer Neural Network, without Pytorch nn.Linear Module\n",
    "  \"\"\"\n",
    "  def __init__(self, \n",
    "               input_size: int, \n",
    "               output_size: int, \n",
    "               activation_fn = lambda x: torch.softmax(x,dim=-1)):\n",
    "      super().__init__()\n",
    "\n",
    "      # randomly initialise the weights with samples from a Gaussian distribution\n",
    "      self._w = nn.Parameter(torch.randn([input_size, output_size], requires_grad = True))\n",
    "\n",
    "      # ... and the biases with zeros\n",
    "      self._b = nn.Parameter(torch.zeros([1, output_size], requires_grad = True))\n",
    "\n",
    "      self._params = [self._w, self._b]\n",
    "      \n",
    "      # define the activation function\n",
    "      self._activation_fn = activation_fn\n",
    "      \n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "      # σ(wx + b)\n",
    "      return self._activation_fn(x @ self._w + self._b)\n",
    "\n",
    "  def zero_grad(self):\n",
    "      \"\"\"Reset the gradients of the parameters\"\"\"\n",
    "      for param in self._params:\n",
    "          if param.grad is not None:\n",
    "              param.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gFnjXYko8GrM"
   },
   "outputs": [],
   "source": [
    "class OneLayer(nn.Module):\n",
    "  \"\"\"Single-Layer Neural Network using Pytorch nn.Linear\"\"\"\n",
    "  def __init__(self, \n",
    "               input_size: int, \n",
    "               output_size: int, \n",
    "               activation_fn = lambda x: torch.softmax(x,dim=-1)):\n",
    "      super().__init__()\n",
    "\n",
    "      # we instantiate a Linear layer\n",
    "      # look up nn.Linear in Pytorch documentation:\n",
    "      #   https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "      self.linear_tr = nn.Linear(\n",
    "          in_features=input_size,\n",
    "          out_features=output_size, \n",
    "          bias=True\n",
    "      )\n",
    "      \n",
    "      # store the activation function\n",
    "      self._activation_fn = activation_fn\n",
    "      \n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "      # step 1: apply linear layer to x\n",
    "      output = self.linear_tr(x)\n",
    "\n",
    "      # step 2: apply softmax to output\n",
    "      y = self._activation_fn(output)\n",
    "\n",
    "      return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7Ab9_itYoxf"
   },
   "source": [
    "With the hard part out of the way, we now just define two helper functions:\n",
    "1. `plot_decision` plots the decision boundary given a model, by predicting the class of every point in the plane\n",
    "2. `plot_loss` simply plots the evolution of our loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pxSmHjLoXhtP"
   },
   "outputs": [],
   "source": [
    "def plot_decision(model: nn.Module, bounds: list = [-8, 8, -8, 8],\n",
    "                  detail: int = 100):\n",
    "    \"\"\"Plot the decision boundary of the `model`\"\"\"\n",
    "\n",
    "    # Get all the points in the region we want to plot\n",
    "    x1, x2 = torch.meshgrid(torch.linspace(bounds[0], bounds[1], detail),\n",
    "                            torch.linspace(bounds[2], bounds[3], detail))\n",
    "    \n",
    "    data = torch.cat((x1.contiguous().view(-1, 1), x2.contiguous().view(-1, 1)),\n",
    "                     dim=1)\n",
    "    \n",
    "    # use the model to predict the class of every point\n",
    "    labels = torch.argmax(model(data), dim=-1)\n",
    "\n",
    "    # plot all of the points\n",
    "    plot_set(data, labels, alpha=0.1)\n",
    "    \n",
    "def plot_loss(loss: list, label: str, color: str = 'blue'):\n",
    "    \"\"\"Plot the evolution of the loss function\"\"\"\n",
    "    plt.plot(loss, label=label, color=color)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWiZqRILZo-t"
   },
   "source": [
    "Let's now plot the computation graph of our network, using a `in_size` of 2 (the `x` and `y` coordinates of every point), and an `out_size` of 2 (the scores the model attributes to the point belonging to each of the 2 classes). To predict the class of the point, we will simply take the `argmax` over the output of the network.\n",
    "\n",
    "We can see from the computation graph all the operations being performed using the weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJzKQJIl5_CD"
   },
   "outputs": [],
   "source": [
    "model = OneLayer(2,2)\n",
    "\n",
    "x = torch.empty(2)\n",
    "torchviz.make_dot(model(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JpXC5Icavuq"
   },
   "source": [
    "Let's now plot the decision boundary of our randomly initialized model. Unless you are very lucky, the model won't explain the data very well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WXijzkJ3a5Jm"
   },
   "outputs": [],
   "source": [
    "plot_decision(model)\n",
    "dist.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kjMoSgNTbOOK"
   },
   "source": [
    "### Model training\n",
    "It's finally time to train our model! First let's generate a sample from our distribution to use as training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "URp7UsRPbq_9"
   },
   "outputs": [],
   "source": [
    "train_data, train_labels = dist.sample(100)\n",
    "plot_set(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3baJY_Cbwcy"
   },
   "source": [
    "Now let's define the loss function that we will use to train our model. We will manually define the Negative Log-Likelihood - NLL (though, in practice, this is already implemented in PyTorch).\n",
    "\n",
    "The network outputs a tensor $\\boldsymbol{p} = (p_1, p_2)$, representing the class likelihoods of each of the two classes. Let $p_i \\in [0, 1]$ be the class score of the correct class. Then the NLL is defined as:\n",
    "$$ \\text{NLL} (\\boldsymbol{p}) = - \\log p_i $$\n",
    "\n",
    "When calculating the loss over the whole data, we will simply compute the mean of the losses over every point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iSOUJ9kTjM40"
   },
   "outputs": [],
   "source": [
    "def NLL(output: torch.Tensor, true_labels: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Given the predictions of the neural network and the ground truth data,\n",
    "    compute the negative log-likelihood.\n",
    "    Arguments:\n",
    "      output: output of neural network, tensor of size N x 2\n",
    "      true_labels: ground truth data, Tensor of size N\n",
    "    \"\"\"\n",
    "\n",
    "    # get the likelihoods of the correct class\n",
    "    likelihood = output.gather(1,true_labels.view(-1,1))\n",
    "\n",
    "    # calculate the mean of loss\n",
    "    loss = -torch.log(likelihood)\n",
    "    loss = torch.mean(loss)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WiXrcrTZ5_Cl"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 400 # the number of epochs to train our model for\n",
    "PRINT_EVERY = 100 # output the results every 100th epoch\n",
    "\n",
    "train_loss = [] # we will store the loss function values at each epoch in this list\n",
    "\n",
    "optim = GD(model.parameters(), lr=0.1) # we will use the `GD` class you defined in Exercise 1 as our optimiser\n",
    "\n",
    "for i in range(NUM_EPOCHS):\n",
    "    output = model(train_data) # run the model over the data to generate the class likelihoods\n",
    "    \n",
    "    loss = NLL(output, train_labels) # compute the loss over the current epoch\n",
    "    \n",
    "    loss.backward() # perform backprop over the loss to compute the gradients of the parameters\n",
    "    with torch.no_grad():\n",
    "      optim.step()      # run an optimisation step\n",
    "      model.zero_grad() # reset the gradients, otherwise they will accumulate\n",
    "    \n",
    "    train_loss.append(loss.detach().numpy()) # store the loss of the current epoch\n",
    "\n",
    "    if i % PRINT_EVERY == 0 or i == NUM_EPOCHS - 1: # every once in a while plot the decision boundary and print the loss\n",
    "        print(f'EPOCH {i}:')\n",
    "        print(f'loss = {loss.item()}')\n",
    "        plot_decision(model)\n",
    "        plot_set(train_data, train_labels)\n",
    "        plt.show()\n",
    "        \n",
    "plot_loss(train_loss, 'train-loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZ_BeLXIugqt"
   },
   "source": [
    "As you can see, by using only one layer, the model is unable to learn decision boundaries more complex than a simple line. Therefore given the XOR distribution below, no `OneLayer` model can successfuly learn it.\n",
    "\n",
    "Note that the XOR distribution is simply formed out of four clusters being assigned 2 different alternating classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RUPFONpZ5_Cv"
   },
   "outputs": [],
   "source": [
    "xor = Clusters(mu = [[-2, -2], [2, 2], [-2, 2], [2, -2]],\n",
    "              sigma = [[1, 1], [1, 1], [1, 1], [1, 1]],\n",
    "              labels = [0, 0, 1, 1])\n",
    "xor.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rm14kEHd5ckW"
   },
   "source": [
    "Because the two classes aren't linearly separable, a Neural Network with a single layer won't manage to learn the XOR distribution. But, for fun, let's actually check that this is true, by instantiating one and training it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GyTpSW7xvUoM"
   },
   "outputs": [],
   "source": [
    "model = OneLayer(2,2)\n",
    "train_data, train_labels = xor.sample(100) # Let's sample our train data from the XOR distribution\n",
    "\n",
    "NUM_EPOCHS = 400\n",
    "PRINT_EVERY = 100\n",
    "\n",
    "train_loss = []\n",
    "\n",
    "optim = GD(model.parameters(), lr=0.1)\n",
    "\n",
    "for i in range(NUM_EPOCHS):\n",
    "    output = model(train_data)\n",
    "    \n",
    "    loss = NLL(output, train_labels)\n",
    "    \n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        optim.step()\n",
    "        model.zero_grad()\n",
    "    \n",
    "    train_loss.append(loss.detach().numpy())\n",
    "\n",
    "    if i % PRINT_EVERY == 0 or i == NUM_EPOCHS - 1:\n",
    "        print(f'EPOCH {i}:')\n",
    "        print(f'loss = {loss.item()}')\n",
    "        plot_decision(model)\n",
    "        plot_set(train_data, train_labels)\n",
    "        plt.show()\n",
    "        \n",
    "plot_loss(train_loss, 'train-loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSaUf3Aktvkf"
   },
   "source": [
    "## Exercise 2: Learning the XOR distribution\n",
    "Implement a multi-layer feed-forward network with 1 hidden layer, using the [``nn.Linear``](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html?highlight=nn%20linear#torch.nn.Linear) modules from Pytorch. This network is capable of learning the XOR distribution. Inspect the decision boundary and the evolution of the loss over the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5MwgqE2A5_DB"
   },
   "outputs": [],
   "source": [
    "class MultiLayer(nn.Module):\n",
    "  \"\"\"Multi-Layer Neural Network (1 hidden layer), without using nn.Linear\"\"\"\n",
    "  def __init__(self, \n",
    "               input_size: int, \n",
    "               hidden_size: int, \n",
    "               output_size: int, \n",
    "               hidden_activation_fn = nn.Tanh(),\n",
    "               output_activation_fn = nn.Softmax(dim=-1)):\n",
    "      \"\"\"\n",
    "      TODO: initialize weights and biases for the hidden and the output layers\n",
    "      Arguments:\n",
    "        input_size: number of input neurons\n",
    "        hidden_size: number of neurons of hidden layer \n",
    "        output_size: number of neurons of output layer\n",
    "      \"\"\"\n",
    "      super().__init__()\n",
    "\n",
    "      # Now we'll have two sets of weights and biases, corresponding to two layers\n",
    "      # 1st layer\n",
    "      self._w1 = nn.Parameter(torch.randn([input_size, hidden_size], requires_grad = True))\n",
    "      self._b1 = nn.Parameter(torch.zeros([1, hidden_size], requires_grad = True))\n",
    "      \n",
    "      # 2nd layer\n",
    "      self._w2 = nn.Parameter(torch.randn([hidden_size, output_size], requires_grad = True))\n",
    "      self._b2 = nn.Parameter(torch.zeros([1, output_size], requires_grad = True))\n",
    "\n",
    "      self._params = [self._w1, self._w2, self._b1, self._b2]\n",
    "\n",
    "      self._hidden_activation_fn = hidden_activation_fn\n",
    "      self._output_activation_fn = output_activation_fn\n",
    "      \n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "      \"\"\"\n",
    "      Feedforward through the two layers.\n",
    "      Arguments:\n",
    "        x: tensor of size (number_of_examples x self.input_size)\n",
    "      \n",
    "      Returns a tensor of size self.output_size, which is the output of the\n",
    "      network after the softmax activation.\n",
    "      \"\"\"\n",
    "      # Layer 1\n",
    "      x = x @ self._w1 + self._b1\n",
    "      x = self._hidden_activation_fn(x)\n",
    "\n",
    "      # Layer 2\n",
    "      x = x @ self._w2 + self._b2\n",
    "      x = self._output_activation_fn(x)\n",
    "\n",
    "      return x\n",
    "  \n",
    "  def zero_grad(self):\n",
    "      \"\"\"Reset the gradients of the parameters\"\"\"\n",
    "      for param in self._params:\n",
    "          if param.grad is not None:\n",
    "              param.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C8UvTwSQGZPc"
   },
   "outputs": [],
   "source": [
    "class MultiLayer(nn.Module):\n",
    "  \"\"\"TODO: Multi-Layer Neural Network (1 hidden layer), using nn.Linear\"\"\"\n",
    "  def __init__(self, \n",
    "               input_size: int, \n",
    "               hidden_size: int, \n",
    "               output_size: int, \n",
    "               hidden_activation_fn = nn.Tanh(),\n",
    "               output_activation_fn = nn.Softmax(dim=-1)):\n",
    "      \"\"\"\n",
    "      TODO: initialize weights and biases for the hidden and the output layers\n",
    "      Arguments:\n",
    "        input_size: number of input neurons\n",
    "        hidden_size: number of neurons of hidden layer \n",
    "        output_size: number of neurons of output layer\n",
    "      \"\"\"\n",
    "      super().__init__()\n",
    "\n",
    "      # TODO: initialize the first linear layer using nn.Linear\n",
    "      self._fst_linear = ...\n",
    "      \n",
    "      # TODO: initialize the second linear layer using nn.Linear\n",
    "      self._snd_linear = ...\n",
    "\n",
    "      self._hidden_activation_fn = hidden_activation_fn\n",
    "      self._output_activation_fn = output_activation_fn\n",
    "      \n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "      \"\"\"\n",
    "      Feedforward through the two layers.\n",
    "      Arguments:\n",
    "        x: tensor of size (number_of_examples x self.input_size)\n",
    "      \n",
    "      Returns a tensor of size self.output_size, which is the output of the\n",
    "      network after the softmax activation.\n",
    "      \"\"\"\n",
    "      # TODO: apply first linear layer, then activation\n",
    "      h = ...\n",
    "\n",
    "      # TODO: apply second linear layer, then activation\n",
    "      o = ...\n",
    "      \n",
    "      return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwMxUp-YuD2t"
   },
   "outputs": [],
   "source": [
    "def NLL(output: torch.Tensor, true_labels: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Given the predictions of the neural network and the ground truth data,\n",
    "    compute the negative log-likelihood.\n",
    "    Arguments:\n",
    "      output: output of neural network, tensor of size N x 2\n",
    "      true_labels: ground truth data, Tensor of size N\n",
    "    \"\"\"\n",
    "    likelihood = output.gather(1, true_labels.view(-1,1))\n",
    "\n",
    "    loss = -torch.log(likelihood)\n",
    "    loss = torch.mean(loss)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2nZOL7Xr66HP"
   },
   "outputs": [],
   "source": [
    "# We need to perform Gradient Descent on 2 layers now, so let's redefine our `GD` class\n",
    "class GD:\n",
    "  def __init__(self, params: torch.Tensor, lr: int):\n",
    "    self.w1, self.w2, self.b1, self.b2 = list(params)\n",
    "    self.lr = lr\n",
    "\n",
    "  def step(self):\n",
    "    with torch.no_grad():\n",
    "      self.w1 -= self.lr * self.w1.grad\n",
    "      self.b1 -= self.lr * self.b1.grad\n",
    "      self.w2 -= self.lr * self.w2.grad\n",
    "      self.b2 -= self.lr * self.b2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_wTm5Q0l4q0U"
   },
   "outputs": [],
   "source": [
    "# For fun, use `torchviz.make_dot()` to see the computation graph of the network\n",
    "INPUT_SIZE = 2\n",
    "HIDDEN_SIZE = 4\n",
    "OUTPUT_SIZE = 2\n",
    "x = torch.randn(INPUT_SIZE)\n",
    "model = MultiLayer(input_size=INPUT_SIZE, \n",
    "                   hidden_size=HIDDEN_SIZE,\n",
    "                   output_size=OUTPUT_SIZE)\n",
    "torchviz.make_dot(model(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eMTuuR3nuDsM"
   },
   "outputs": [],
   "source": [
    "# Finally, train the network and plot the decision boundaries\n",
    "model = MultiLayer(input_size=INPUT_SIZE, \n",
    "                   hidden_size=HIDDEN_SIZE,\n",
    "                   output_size=OUTPUT_SIZE)\n",
    "train_data, train_labels = xor.sample(100) # Let's sample our train data from the XOR distribution\n",
    "\n",
    "NUM_EPOCHS = 400\n",
    "PRINT_EVERY = 100\n",
    "\n",
    "train_loss = []\n",
    "optim = GD(model.parameters(), lr=0.1)\n",
    "\n",
    "for i in range(NUM_EPOCHS):\n",
    "    output = model(train_data)\n",
    "    loss = NLL(output, train_labels)\n",
    "    \n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    model.zero_grad()\n",
    "    \n",
    "    train_loss.append(loss.detach().numpy())\n",
    "\n",
    "    if i % PRINT_EVERY == 0 or i == NUM_EPOCHS - 1:\n",
    "        print(f'EPOCH {i}:')\n",
    "        print(f'loss = {loss.item()}')\n",
    "        plot_decision(model)\n",
    "        plot_set(train_data, train_labels)\n",
    "        plt.show()\n",
    "        \n",
    "plot_loss(train_loss, 'train-loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdGwOkTWdq5l"
   },
   "source": [
    "## Exercise 3: Learning three clusters\n",
    "\n",
    "Define a Neural Network that can successfully classify samples belonging to these three clusters:\n",
    "\n",
    "![3 clusters in a plane](https://i.imgur.com/EnFKfZG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ltL-yKWeHbz"
   },
   "source": [
    "First up, let's define our clusters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z-qqu3hQeLTp"
   },
   "outputs": [],
   "source": [
    "threeClass = Clusters(mu = [[-2,-2],[2,2],[-2,2]],\n",
    "                      sigma = [[1,1],[1,1],[1,1]],\n",
    "                      labels = [0,1,2])\n",
    "threeClass.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xE3oGD63eMsj"
   },
   "source": [
    "Finally, let's sample a train dataset and a validation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JTDZTMP8eRyW"
   },
   "outputs": [],
   "source": [
    "train_data, train_labels = threeClass.sample(200)\n",
    "val_data, val_labels = threeClass.sample(50)\n",
    "\n",
    "plot_set(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQI2-DTOeeI_"
   },
   "source": [
    "You should:\n",
    "1. Define the Neural Network, inheriting `torch.nn.Module` and using all the appropriate objects provided by torch. You can use one of the loss functions already provided by `torch.nn.functional` [[5]](https://pytorch.org/docs/stable/nn.functional.html);\n",
    "2. For fun, plot the computation graph;\n",
    "3. Train the network like before, printing the loss and decision boundary from time to time. For now, you can use the `torch.optim.SGD` optimiser [[6]](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD);\n",
    "4. Take a sample from the distribution and use it as a validation dataset, testing the loss function on it without training every epoch. Plot both losses;\n",
    "5. Compare the model's training using `torch.optim.SGD` vs. `torch.optim.Adam` [[7]](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PpPw4R5PW9X6"
   },
   "outputs": [],
   "source": [
    "class ThreeClassNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size: int, \n",
    "                 hidden_size: int, \n",
    "                 output_size: int,\n",
    "                 hidden_activation_fn = nn.ReLU()):\n",
    "        \"\"\"\n",
    "        TODO: initialize a multi-layer NN with 1 hidden layer using the\n",
    "        pytorch package torch.nn.Linear\n",
    "        \"\"\"\n",
    "        # Initialise the base class (nn.Module)\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO use `torch.nn.Linear` to instantiate the hidden and the output\n",
    "        # layer. Look up the Pytorch documentation:\n",
    "        #   https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "\n",
    "        self._layer1 = ...\n",
    "        self._layer2 = ...\n",
    "\n",
    "        self._hidden_activation = hidden_activation_fn\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # TODO: apply first layer transformation + activation\n",
    "        h = ...\n",
    "\n",
    "        # TODO: apply second layer transformation\n",
    "        # Because we will use CrossEntropy as our loss, we don't need a\n",
    "        # a softmax activation function after layer 2\n",
    "        out = ...\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sn1CnRpyfJ_7"
   },
   "outputs": [],
   "source": [
    "# For fun, use `torchviz.make_dot()` to see the computation graph of the network\n",
    "model = ThreeClassNN(2, 4, 3)\n",
    "x = torch.randn(2)\n",
    "torchviz.make_dot(model(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F4mWV646fKAd"
   },
   "outputs": [],
   "source": [
    "# Finally, train the network and plot the decision boundaries\n",
    "model = ThreeClassNN(2, 4, 3)\n",
    "\n",
    "NUM_EPOCHS = 400\n",
    "PRINT_EVERY = 50\n",
    "\n",
    "train_loss = []\n",
    "val_loss = [] # This time we will track the loss on val_data\n",
    "\n",
    "# TODO: instantiante SGD optimizer with a learning rate of 0.01\n",
    "# look up the Pytorch documentation:\n",
    "# https://pytorch.org/docs/stable/optim.html?highlight=sgd#torch.optim.SGD\n",
    "optim = ...\n",
    "\n",
    "# TODO: instantiate the Cross Entropy loss\n",
    "# look up the Pytorch documentation:\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "criterion = ...\n",
    "\n",
    "for i in range(NUM_EPOCHS):\n",
    "    # Set the model to train mode and reset the gradients\n",
    "    model.train()\n",
    "    optim.zero_grad()\n",
    "\n",
    "    output = model(train_data)\n",
    "    \n",
    "    # The main difference between Cross Entropy and NLL is that the first doesn't\n",
    "    # expect the output to be class likelihoods (\\in [0, 1]), but rather\n",
    "    # class scores (\\in \\mathbb{R}). That's why we didn't use softmax this time\n",
    "    # on the last layer.\n",
    "    loss = criterion(output, train_labels)\n",
    "    \n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    train_loss.append(loss.detach().numpy())\n",
    "\n",
    "    if i % PRINT_EVERY == 0 or i == NUM_EPOCHS - 1:\n",
    "        print(f'EPOCH {i}:')\n",
    "        print(f'loss = {loss.item()}')\n",
    "        plot_decision(model)\n",
    "        plot_set(train_data, train_labels)\n",
    "        plt.show()\n",
    "\n",
    "    # Every epoch, let's evaluate our model on the validation data and plot both losses at the end\n",
    "    model.eval() # set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        output = model(val_data)\n",
    "        validation_loss = F.cross_entropy(output, val_labels)\n",
    "        val_loss.append(validation_loss)\n",
    "\n",
    "plot_loss(train_loss, 'train-loss')\n",
    "plot_loss(val_loss, 'val-loss', color='green')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lab_2_(unsolved) - 2021.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
