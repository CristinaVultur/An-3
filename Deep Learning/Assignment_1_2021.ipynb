{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zR1ZlT9j0U4p"
   },
   "source": [
    "# Assignment 1: Bucharest Housing Dataset\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfXso_GoZk9u"
   },
   "source": [
    "## Dataset Description\n",
    "In the dataset linked below you have over three thousand apartments listed for sale on the locally popular website *imobiliare.ro*. Each entry provides details about different aspects of the house or apartment:\n",
    "1. `Nr Camere` indicates the number of rooms;\n",
    "2. `Suprafata` specifies the total area of the dwelling;\n",
    "3. `Etaj` specifies the floor that the home is located at;\n",
    "4. `Total Etaje` is the total number of floors of the block of flats;\n",
    "5. `Sector` represents the administrative district of Bucharest in which the apartment is located;\n",
    "6. `Pret` represents the listing price of each dwelling;\n",
    "7. `Scor` represents a rating between 1 and 5 of location of the apartment. It was computed in the following manner by the dataset creator:\n",
    "  1. The initial dataset included the address of each flat;\n",
    "  2. An extra dataset was used, which included the average sales price of dwellings in different areas of town;\n",
    "  3. Using all of these monthly averages, a clusterization algorithm grouped them into 5 classes, which were then labelled 1-5;\n",
    "  4. You can think of these scores as an indication of the value of the surrounding area, with 1 being expensive, and 5 being inexpensive.\n",
    "\n",
    "Dataset Source: [kaggle.com/denisadutca](https://www.kaggle.com/denisadutca/bucharest-house-price-dataset/kernels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SwVnR01-ZmIE"
   },
   "source": [
    "## To Do\n",
    "\n",
    "To complete this assignment, you must:\n",
    "1. Get the data in a PyTorch-friendly format;\n",
    "2. Predict the `Nr Camere` of each dwelling, treating it as a **classification** problem. Choose an appropriate loss function;\n",
    "3. Predict the `Nr Camere` of each dwelling, treating it as a **regression** problem. Choose an appropriate loss function;\n",
    "4. Compare the results of the two approaches, displaying the Confusion Matrix for the two, as well as any comparing any other metrics you think are interesting (e.g. MSE). Comment on the results;\n",
    "5. Choose to predict a feature more suitable to be treated as a **regression** problem, then successfully solve it.\n",
    "6. What values should the loss have when the predictions are random (when your network is not trained at all)?\n",
    "7. Don't forget to split the dataset in training and validation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "noDnc4PEZnOk"
   },
   "source": [
    "## Hints\n",
    "1. It might prove useful to link your Google Drive to this Notebook. See the code cell below;\n",
    "2. You might want to think of ways of preprocessing your data (e.g. One Hot Encoding, etc.);\n",
    "3. Don't be afraid of using text cells to actually write your thoughts about the data/results. Might prove useful at the end of the semester when you'll need to walk us through your solution ðŸ˜‰.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Pjwrt_IZoeQ"
   },
   "source": [
    "## Deadline\n",
    "March 18, 2021, 23:59\n",
    "\n",
    "**Punctaj maxim:** 2 puncte.\n",
    "\n",
    "Depunctarea este de 0.25 puncte pe zi intarziata. Dupa mai mult de 4 zile intarziere, punctajul maxim care se poate obtine ramane 1 punct.\n",
    "\n",
    "Trimite notebookul si datasetul intr-o arhiva `NumePrenume_Grupa_Tema1.zip` aici: https://forms.gle/MGrLvehEjmtWmQZP7 (la sustinerea temei, vei rula codul din arhiva)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from typing import Iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Predict the Nr Camere of each dwelling, treating it as a regression problem. Choose an appropriate loss function.\n",
    "Use Linear Regression from Lab2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE():\n",
    "    \"\"\"The Mean Squared Error loss\"\"\"\n",
    "    def __call__(self, y: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        y = torch.Tensor(y)\n",
    "        target = torch.Tensor(target)\n",
    "        mse = ((y - target) ** 2).sum().sqrt().mean()\n",
    "        return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDLinearRegression(nn.Module):\n",
    "    \"\"\"A simple Linear Regression model\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # We're initializing our model with random weights\n",
    "        self.w = nn.Parameter(torch.randn(6, requires_grad=True))\n",
    "        self.b = nn.Parameter(torch.randn(1, requires_grad=True))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.Tensor(x)\n",
    "        y = x @ self.w + self.b\n",
    "        return y\n",
    "\n",
    "  # PyTorch is accumulating gradients\n",
    "  # After each Gradient Descent step we should reset the gradients\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.Tensor(x)\n",
    "        y = x @ self.w + self.b\n",
    "        return y\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GD:\n",
    "  \"\"\"\n",
    "  Gradient Descent optimizer\n",
    "  \"\"\"\n",
    "  def __init__(self, params: Iterator[nn.Parameter], lr: int):\n",
    "    self.w, self.b = list(params)\n",
    "    self.lr = lr\n",
    "\n",
    "  def step(self):\n",
    "    \"\"\"\n",
    "    Perform a gradient decent step. Update the parameters w and b by using:\n",
    "     - the gradient of the loss with respect to the parameters\n",
    "     - the learning rate\n",
    "    This method is called after backward(), so the gradient of the loss wrt\n",
    "    the parameters is already computed.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "      self.w -= self.w.grad * self.lr\n",
    "      self.b -= self.b.grad * self.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: GDLinearRegression, data: torch.Tensor,\n",
    "          target: torch.Tensor, optim: GD, criterion: MSE):\n",
    "    \"\"\"Linear Regression train routine\"\"\"\n",
    "    # forward pass: compute predictions (hint: use model.forward)\n",
    "    predictions = model(data)\n",
    "\n",
    "    # forward pass: compute loss (hint: use criterion)\n",
    "    loss = criterion(predictions, target)\n",
    "\n",
    "    # backpropagation: compute gradients of loss wrt weights\n",
    "    loss.backward()\n",
    "\n",
    "    # GD step: update weights using the gradients (hint: use optim)\n",
    "    optim.step()\n",
    "\n",
    "    # reset the gradients (hint: use model)\n",
    "    model.zero_grad()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Get the data in a PyTorch-friendly format\n",
    "Split the data in traing and validation and tandardize features by removing the mean and scaling to unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Bucharest_HousePriceDataset.csv', sep=',')\n",
    "\n",
    "predict = 'Nr Camere'\n",
    "x = data.drop(columns=predict).values\n",
    "\n",
    "y = data[[predict]].values.ravel()\n",
    "y = y-1\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = MSE()\n",
    "lr = 0.0015 #@param {type: \"slider\", min: 0.001, max: 2, step: 0.005}\n",
    "total_steps = 100 #@param {type:\"slider\", min: 0, max: 100, step: 1}\n",
    "\n",
    "model = GDLinearRegression()\n",
    "optimizer = GD(model.parameters(), lr=lr)\n",
    "criterion = MSE()\n",
    "\n",
    "for i in range(total_steps):\n",
    "    train(model, x_train, y_train, optimizer, criterion)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the accuracy for training and validation, as well as the Confusion Matrix and MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.683669854764435\n",
      "0.6883852691218131\n",
      "[[  6   0   0   0   0   0   0]\n",
      " [ 54 313  71   1   0   0   0]\n",
      " [  0  14 145  42   0   0   0]\n",
      " [  0   0  15  21   5   0   0]\n",
      " [  0   0   1   8   1   1   0]\n",
      " [  0   0   1   2   2   0   0]\n",
      " [  0   0   1   2   0   0   0]]\n",
      "MSE: tensor(16.4621)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_train, torch.round(y_pred).numpy()))\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(x_test)\n",
    "\n",
    "print(accuracy_score(y_test, torch.round(y_pred).numpy()))\n",
    "\n",
    "predicted = torch.round(y_pred).numpy()\n",
    "regression_matrix = confusion_matrix(predicted,y_test)\n",
    "regression_mse = mse(predicted,y_test)\n",
    "\n",
    "print(regression_matrix)\n",
    "print('MSE:', regression_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the Nr Camere of each dwelling, treating it as a \n",
    "classification problem. \n",
    "Choose an appropriate loss function\n",
    "Using the nn from lab2 ex 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define out Nn\n",
    "class ThreeClassNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size: int,\n",
    "                 hidden_size: int,\n",
    "                 output_size: int,\n",
    "                 hidden_activation_fn=nn.ReLU()):\n",
    "        # Initialise the base class (nn.Module)\n",
    "        super().__init__()\n",
    "\n",
    "        # As stated above, we'll simply use `torch.nn.Linear` to define our 2 layers\n",
    "        self._layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self._layer2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self._hidden_activation = hidden_activation_fn\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Layer 1 using ReLU as activation\n",
    "        h = self._hidden_activation(self._layer1(x))\n",
    "        out = self._layer2(h)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = 'Nr Camere'\n",
    "\n",
    "x = data.drop(columns=predict).values\n",
    "\n",
    "y = data[[predict]].values\n",
    "y = y-1\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x)\n",
    "x_train = torch.tensor(scaler.transform(x_train)).float()\n",
    "x_test = torch.tensor(scaler.transform(x_test)).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ThreeClassNN(6, 1000, 9)\n",
    "\n",
    "NUM_EPOCHS = 500\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for i in range(NUM_EPOCHS):\n",
    "    # Set the model to train mode and reset the gradients\n",
    "    model.train()\n",
    "    optim.zero_grad()\n",
    "\n",
    "    output = model(x_train)\n",
    "    target = torch.tensor(y_train).long().squeeze(1)\n",
    "    loss = criterion(output, target)\n",
    "\n",
    "    loss.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the accuracy for training and validation, as well as the Confusion Matrix and MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7357421183138505\n",
      "0.6841359773371105\n",
      "[[ 51  10   0   0   0   0   0]\n",
      " [ 27 233  53   5   0   0   0]\n",
      " [  0  54 180  45   2   1   0]\n",
      " [  0   2  20  19   0   0   0]\n",
      " [  0   0   0   3   0   0   0]\n",
      " [  0   0   0   0   0   0   0]\n",
      " [  0   0   0   1   0   0   0]]\n",
      "MSE: tensor(815.0227)\n"
     ]
    }
   ],
   "source": [
    "predicted = np.array(torch.argmax(model(x_train), dim=-1))\n",
    "accuracy = (predicted==y_train.ravel()).sum()/predicted.shape[0]\n",
    "print(accuracy)\n",
    "\n",
    "predicted = np.array(torch.argmax(model(x_test), dim=-1))\n",
    "accuracy = (predicted==y_test.ravel()).sum()/predicted.shape[0]\n",
    "print(accuracy)\n",
    "\n",
    "\n",
    "class_matrix = confusion_matrix(predicted,y_test)\n",
    "class_mse = mse(predicted,y_test)\n",
    "\n",
    "print(class_matrix)\n",
    "print('MSE:', class_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss when the network is not trained.\n",
    "Random variables uniform distribuited.\n",
    "Random  means that any number can occur with the same probability so the prob to be a certain number\n",
    "of rooms is 1/max number (max number = 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1972245683362193\n"
     ]
    }
   ],
   "source": [
    "p = 1.0/9.0\n",
    "Q = [p,p,p,p,p,p,p,p,p]\n",
    "\n",
    "#calculate the probability for each nr of rooms from our \n",
    "#dataset nr_favourite_cases/nr of posible cases\n",
    "\n",
    "nr_y = [0,0,0,0,0,0,0,0,0]\n",
    "for i in range(len(y)):\n",
    "    nr_y[y[i][0]] += 1\n",
    "\n",
    "P = [nr_y[i]/len(y) for i in range(len(nr_y))]\n",
    "\n",
    "\n",
    "#Cross-entropy can be calculated using the probabilities of\n",
    "#the events from P and Q, as follows:\n",
    "#H(P, Q) = â€“ sum x in X P(x) * log(Q(x))\n",
    "H = 0\n",
    "for i in range(len(Q)):\n",
    "    H += P[i] * np.log(Q[i]+1e-9) #we don't want 0 values\n",
    "    \n",
    "H = -H\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose to predict a feature more suitable to be treated as a regression problem, then successfully solve it.\n",
    "Choose: pret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = 'Pret'\n",
    "\n",
    "x = data.drop(columns=predict).values\n",
    "\n",
    "y = data[[predict]].values.ravel()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 100\n",
    "total_steps = 100\n",
    "model = GDLinearRegression()\n",
    "optimizer = GD(model.parameters(), lr=lr)\n",
    "criterion = MSE()\n",
    "\n",
    "for i in range(total_steps):\n",
    "    train(model, x_train, y_train, optimizer, criterion)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traing and validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1650838.75\n",
      "852138.875\n"
     ]
    }
   ],
   "source": [
    "mse = MSE()\n",
    "print(mse(y_pred, y_train).item())\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(x_test)\n",
    "\n",
    "mse = MSE()\n",
    "print(mse(y_pred, y_test).item())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_1_2021.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
