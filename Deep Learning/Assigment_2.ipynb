{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 1: Without training on the counting on dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from IPython.core.debugger import set_trace\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# we need google drive access to upload the datasets\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the network from lab 3 used for classification of MNIST and using it for our counting problem\n",
    "\n",
    "# > MNIST classification\n",
    "\n",
    "\n",
    "---\n",
    "We will train a convolutional network to classify digits. The architecture should be similar to:\n",
    "  - conv layer: 20 filters, kernel size: 5x5, stride:1\n",
    "  - relu\n",
    "  - max pool: kernel size: 2x2, stride:2\n",
    "  - conv layer: 50 filters, kernel size: 5x5, stride:1\n",
    "  - relu\n",
    "  - max pool: kernel size: 2x2, stride:2\n",
    "  - fully connected: 500 neurons\n",
    "  - relu\n",
    "  - fully connected: 10 neurons\n",
    "  - log softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings    \n",
    "kwargs={}\n",
    "class Args():\n",
    "  def __init__(self):\n",
    "      self.batch_size = 64\n",
    "      self.test_batch_size = 64\n",
    "      self.epochs = 10\n",
    "      self.lr = 0.01\n",
    "      self.momentum = 0.9\n",
    "      self.seed = 1\n",
    "      self.log_interval = int(10000 / self.batch_size)\n",
    "      self.cuda = False\n",
    "\n",
    "args = Args()\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(args.seed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_filters1 = 20\n",
    "no_filter2 = 50\n",
    "no_neurons1 = 500\n",
    "class CNN(nn.Module):\n",
    "    # the init() is called a single time, when you create the model\n",
    "    # so all the layers should be created here.\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = no_filters1, kernel_size = 5, stride = 1)\n",
    "        self.conv2 = nn.Conv2d(no_filters1, no_filter2, 5, 1)\n",
    "        self.fc1 = nn.Linear(in_features = 4 * 4 * no_filter2, out_features = no_neurons1)\n",
    "        self.fc2 = nn.Linear(in_features = no_neurons1, out_features = 10)\n",
    "    # the forward() is called at each iteration, so we only apply the already\n",
    "    # created operations inside this function \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*no_filter2)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load MNIST dataset containing drawn digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
    "!tar -zxvf MNIST.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a loader to iterate through the dataset\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       \n",
    "                   ])),\n",
    "    batch_size=args.batch_size, shuffle=True,drop_last=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                   ])),\n",
    "    batch_size=args.test_batch_size, shuffle=False,drop_last=True, **kwargs)\n",
    "\n",
    "first_train_batch_imgs, first_train_batch_labels = next(iter(train_loader))\n",
    "# set_trace()\n",
    "\n",
    "f, axarr = pyplot.subplots(1,5)\n",
    "for i in range(5):\n",
    "  axarr[i].imshow(first_train_batch_imgs[i,0])\n",
    "print(f'Labels of the shown images: {first_train_batch_labels[:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Convolutional network for MNIST classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define two functions, one for training the model and one for testing it\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    all_losses = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # put the data on the GPU\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # initialize as zeros all the gradients of the model\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # obtain the predictions in the FORWARD pass of the network\n",
    "        output = model(data)\n",
    "        # compute average LOSS for the current batch\n",
    "        loss = F.nll_loss(output, target)\n",
    "        all_losses.append(loss.detach().cpu().numpy())\n",
    "        # BACKPROPAGATE the gradients\n",
    "        loss.backward()\n",
    "        # use the computed gradients to OPTIMISE the model\n",
    "        optimizer.step()\n",
    "        # print the training loss of each batch\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    return np.array(all_losses).mean()\n",
    "\n",
    "def test(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        num_iter = 0\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # obtain the prediction by a forward pass\n",
    "            output = model(data)\n",
    "            # calculate the loss for the current batch and add it across the entire dataset\n",
    "            test_loss += F.nll_loss(output, target) # sum up batch loss\n",
    "            # compute the accuracy of the predictions across the entire dataset\n",
    "            # get the most probable prediction\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).float().mean().item()\n",
    "            num_iter += 1\n",
    "    test_loss /= num_iter\n",
    "    test_accuracy = 100. * correct / num_iter\n",
    "    # print the Accuracy for the entire dataset\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: ({:.0f}%)\\n'.format(\n",
    "        test_loss,\n",
    "        test_accuracy))\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an optimizer and call the training / testing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(loss, label, color='blue'):\n",
    "    pyplot.plot(loss, label=label, color=color)\n",
    "    pyplot.legend()\n",
    "\n",
    "# move the model to the GPU (when available)\n",
    "model = CNN().to(device)\n",
    "# create an Stochastic Gradient Descent optimiser\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "\n",
    "losses_train = []\n",
    "losses_test = []\n",
    "accuracy_test = []\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "# for epoch in range(1, 3):\n",
    "    train_loss = train(args, model, device, train_loader, optimizer, epoch)\n",
    "    test_loss, test_accuracy = test(args, model, device, test_loader)\n",
    "    losses_train.append(train_loss)\n",
    "    losses_test.append(test_loss)\n",
    "    accuracy_test.append(test_accuracy)\n",
    "\n",
    "#plot the loss/accuracy    \n",
    "pyplot.figure(1)\n",
    "plot_loss(losses_train,'train_loss','red')\n",
    "plot_loss(losses_test,'test_loss')\n",
    "pyplot.figure(2)\n",
    "plot_loss(accuracy_test,'test_accuracy')\n",
    "\n",
    "# save the final model\n",
    "torch.save(model.state_dict(),\"mnist_cnn.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# > MNIST counting\n",
    "\n",
    "\n",
    "---\n",
    "Load loalisation dataset.\n",
    "Our goal is to count the digits in every image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def get_large_dataset(path, max_batch_idx=100, shuffle=False,first_k=5000):\n",
    "  # load the dataset as numpy arrays (tensors)\n",
    "  with open(path,'rb') as handle:\n",
    "    data = pickle.load(handle)\n",
    "  # select only first_k elements in the dataset\n",
    "  np_dataset_large  = np.expand_dims(data['images'],1)[:first_k]\n",
    "  np_dataset_coords = data['coords'].astype(np.float32)[:first_k]\n",
    "  \n",
    "  # show a couple of examples from the dataset\n",
    "  print(f'np_dataset_large shape: {np_dataset_large.shape}')\n",
    "  for ii in range(5):\n",
    "    example = np_dataset_large[10+ii].reshape((100, 100))\n",
    "    pyplot.figure()\n",
    "    pyplot.imshow(example, cmap=\"gray\")\n",
    "  \n",
    "  # create loader from the numpy tensors\n",
    "  from torch.utils.data import TensorDataset\n",
    "  from torch.utils.data import DataLoader\n",
    "  dataset_large, dataset_coords = map(torch.tensor, \n",
    "                (np_dataset_large, np_dataset_coords))\n",
    "  dataset_large = dataset_large.to(device)\n",
    "  dataset_coords = dataset_coords.to(device)\n",
    "\n",
    "  large_dataset = TensorDataset(dataset_large, dataset_coords)\n",
    "  large_data_loader = DataLoader(large_dataset, \n",
    "       batch_size=args.batch_size, shuffle=shuffle, drop_last=True)\n",
    "  return large_data_loader\n",
    "\n",
    "# create both train and test dataset\n",
    "# TODO: change these paths to the place where the pickles are stored in you drive\n",
    "path_train = '/content/gdrive/MyDrive/Copy of data_train.pickle'\n",
    "path_test = '/content/gdrive/MyDrive/Copy of data_test.pickle'\n",
    "\n",
    "large_data_loader_train = get_large_dataset(path_train,max_batch_idx=50,shuffle=True, first_k=1000)\n",
    "large_data_loader_test = get_large_dataset(path_test,max_batch_idx=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will convert the network a fully convolutional network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design the fully convolutional network   \n",
    "# the first two conv layers should be the same as the original classification conv layers\n",
    "# the last two conv layers should be transformed from the last two fully connected layers in the original network\n",
    "class CNN_fully_conv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, no_filters1, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(no_filters1, no_filter2, 5, 1)\n",
    "        self.fully_conv1  = nn.Conv2d(no_filter2,no_neurons1, 4)\n",
    "        self.fully_conv2 = nn.Conv2d(no_neurons1,5, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.fully_conv1(x))\n",
    "        x = self.fully_conv2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run this network on the larger 100x100 images and get 10 maps representing the probability that a digit is found at that location. For every of the 5 feature maps (one for every posible count), we will select the feature map with the strongest activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the old classification model, and create a new fully convolutional model from the old parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data should be in [0,1]\n",
    "def preprocess(data):\n",
    "  return data.float() / 255.0\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "# save the classification model\n",
    "PATH = 'conv_net.pt'\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "# define the fully_conv model\n",
    "model_fuly_conv = CNN_fully_conv()\n",
    "\n",
    "# load classification model\n",
    "loaded_state_dict = torch.load(PATH)\n",
    "\n",
    "# loaded_state_dict contain the weights of the classification model\n",
    "# For the fully_conv model we will use exactly the same parameters.\n",
    "# For the convolutional part we can directly load them as they have the same name.\n",
    "\n",
    "# We need to convert the last fully-connected layers into convolutions\n",
    "# For a single neuron we would just reshape the parameters from a vector into a kernel. \n",
    "# We just need to know what is the spatial dimension of the original fully-connected input.\n",
    "# In our case, the first fully-connected had an input of size 4 x 4 x no_filter2, so we must use kernels of size 4 x 4 x no_filter2\n",
    "# The second fully-connected receives as input just a vector(1x1 spatial dimension) of size no_neurons1 thus we use kernels of size 1 x 1 x no_neurons1\n",
    "model_dict = {}\n",
    "for key,val in loaded_state_dict.items():\n",
    "  key = key.replace('fc','fully_conv')\n",
    "  print(f'key: {key}')\n",
    "  if 'fully_conv1.weight' in key:\n",
    "    val = val.view(-1,no_filter2,4,4)\n",
    "  if 'fully_conv2.weigh' in key:\n",
    "    val = val.view(-1,no_neurons1,1,1)\n",
    "  model_dict[key] = val\n",
    "  \n",
    "model_fuly_conv.load_state_dict(model_dict)\n",
    "model_fuly_conv = model_fuly_conv.to(device)\n",
    "\n",
    "print(model_fuly_conv.parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate this fully convolutional network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over all the batches and estimate the location of the digit for each sample\n",
    "for batch_idx, (large_imgs, target_coords) in enumerate(large_data_loader_test):\n",
    "  print(f'large_imgs shape {large_imgs.shape}')\n",
    "  large_imgs = preprocess(large_imgs)\n",
    "  out_prob_maps = model_fuly_conv(large_imgs)\n",
    "  # from the 5 maps, we select the index of the strongest activation\n",
    "  max_ind = torch.argmax(torch.max(out_prob_maps.view(args.batch_size,5,-1),dim=2)[0],dim=1)\n",
    "  # just for the first batch lets print some of the feature maps:\n",
    "  if batch_idx == 0:\n",
    "    for ii in range(5):\n",
    "      pyplot.figure()\n",
    "      pyplot.imshow(large_imgs[ii,0].cpu().detach().numpy(), cmap=\"gray\")\n",
    "\n",
    "      pyplot.figure()\n",
    "      # pyplot.imshow(np.log(out_prob_maps[ii,max_ind[ii]].cpu().detach().numpy()), cmap=\"gray\")\n",
    "      pyplot.imshow(out_prob_maps[ii,max_ind[ii]].cpu().detach().numpy(), cmap=\"jet\")\n",
    "      pyplot.colorbar()\n",
    "      \n",
    "  all_locs = []\n",
    "  # get the location of the maxim, for every example in batch\n",
    "  # for this we linearise the selected map into a vector and find the maximum\n",
    "  for i in range(args.batch_size):\n",
    "    max_loc = torch.argmax(out_prob_maps[i,max_ind[i]])\n",
    "    all_locs.append(max_loc)\n",
    "  max_location = torch.stack(all_locs,dim=0)\n",
    "  # compute the index in the original map from the index in the vector\n",
    "  max_location_x = torch.unsqueeze(max_location / 19, dim=1).float()  / 19.0 \n",
    "  max_location_y = torch.unsqueeze(max_location % 19, dim=1).float() / 19.0 \n",
    "  coords = torch.cat([max_location_x,max_location_y],dim=1)\n",
    "  # compute the error between the estimated location and the ground truth one\n",
    "  mse = torch.mean(torch.sqrt(torch.sum((target_coords - coords) * (target_coords - coords),dim=1)))\n",
    "  print(f'Mean squared error: {mse}')\n",
    "  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
